{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 20\n",
    "class data(object):\n",
    "    def __init__(self):\n",
    "        self.IP = []\n",
    "        self.cert = []\n",
    "        self.urls = []\n",
    "        self.static_features = []\n",
    "        self.seq = []\n",
    "        self.label = []\n",
    "        self.time = []\n",
    "        self.image = []\n",
    "        self.fn = []\n",
    "        \n",
    "    def insert(self,IP,cert,urls,static_features,seq,label,time,image,fn):\n",
    "        self.IP.append(IP)\n",
    "        self.cert.append(cert)\n",
    "        self.urls.append(urls)\n",
    "        self.static_features.append(static_features)\n",
    "        self.seq.append(seq)\n",
    "        self.label.append(label)\n",
    "        self.time.append(time)\n",
    "        self.image.append(image)\n",
    "        self.fn.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interval import Interval\n",
    "import os,shutil\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "class cert_cluster(object):\n",
    "    def __init__(self):\n",
    "        self.IPs = []\n",
    "        self.urls = {} #{urls:number}\n",
    "        self.url_number = [] # sorted (url:number)\n",
    "        self.url_weight = {}#{url:weight}\n",
    "        self.url_unique = []\n",
    "        self.certs = [] \n",
    "        self.sessions = []\n",
    "        self.images = []\n",
    "        self.static_features = []\n",
    "        self.fn = []\n",
    "        \n",
    "\n",
    "        self.seq = []\n",
    "        self.label = -1\n",
    "        self.tag = -1\n",
    "        self.time_slides = [] #(start,end)\n",
    "        \n",
    "        self.sim_list = []\n",
    "        self.url_all = []\n",
    "        \n",
    "        self.ip_weight = {}\n",
    "        self.cert_weight = {}\n",
    "        \n",
    "    #file name\n",
    "    def fn_cal(self):\n",
    "        self.fn = [ss['fn'] for ss in self.sessions]\n",
    "            \n",
    "        \n",
    "    #all urls\n",
    "    def all_url_cal(self):\n",
    "        for session in self.sessions:\n",
    "            urls = session['urls']\n",
    "            for url in urls:\n",
    "                if url not in self.url_all:\n",
    "                    self.url_all.append(url)\n",
    "    #url sim cal\n",
    "    def sim_cal(self,urls_dict):\n",
    "        self.sim_list = []\n",
    "        for url_dict_per in urls_dict:\n",
    "            score = 0\n",
    "            for url in self.url_unique:\n",
    "                if url in url_dict_per:\n",
    "                    score += 1\n",
    "            self.sim_list.append(score / (len(self.url_unique)*1.0))\n",
    "        if np.array(self.sim_list).max() > 0:\n",
    "            self.sim_list = self.sim_list/np.array(self.sim_list).max()\n",
    "        max_idx = np.argmax(np.array(self.sim_list))\n",
    "        for i in range(len(self.sim_list)):\n",
    "            if i!=max_idx:\n",
    "                self.sim_list[i] = 0\n",
    "            \n",
    "    #ip sim cal\n",
    "    def sim_ip_cal(self,urls_dict):\n",
    "        self.sim_list = []\n",
    "        for url_dict_per in urls_dict:\n",
    "            score = 0\n",
    "            for url in self.IPs:\n",
    "                if url in url_dict_per:\n",
    "                    score += 1\n",
    "            self.sim_list.append(score)\n",
    "        if np.array(self.sim_list).max() > 0:\n",
    "            self.sim_list = self.sim_list/np.array(self.sim_list).max()\n",
    "        max_idx = np.argmax(np.array(self.sim_list))\n",
    "        for i in range(len(self.sim_list)):\n",
    "            if i!=max_idx:\n",
    "                self.sim_list[i] = 0\n",
    "        \n",
    "            \n",
    "    \n",
    "    #static\n",
    "    def static_cal(self):\n",
    "        self.static_features = [a['static_features'] for a in self.sessions]\n",
    "    \n",
    "    \n",
    "    #major voting for cluster's label\n",
    "    def label_cal(self):\n",
    "        labels = [a['label'] for a in self.sessions]\n",
    "        self.label = max(labels,key = labels.count)\n",
    "        \n",
    "    def tag_cal(self):\n",
    "        tags = [a['tag'] for a in self.sessions]\n",
    "        self.tag = max(tags,key = tags.count)\n",
    "    \n",
    "    #image extract\n",
    "    def image_cal(self):\n",
    "        self.images = [item['image'].flatten() for item in self.sessions]\n",
    "    \n",
    "    #seq_ipdaate\n",
    "    def seq_cal(self):\n",
    "#         self.seq = [item['seq'].flatten() for item in self.sessions]\n",
    "        for i in range(len(self.sessions)):\n",
    "            session = self.sessions[i]\n",
    "            mat = session['seq']\n",
    "            sta = session['static_features']\n",
    "            for item in sta:\n",
    "                mat = np.append(mat,float(item))\n",
    "\n",
    "            self.seq.append(mat)\n",
    "            \n",
    "            \n",
    "        \n",
    "    #time_slieds generation\n",
    "    def time_cal(self):\n",
    "        def getfirst(item):\n",
    "            return item[0]\n",
    "        for session in self.sessions:\n",
    "            self.time_slides.append(session['time'])\n",
    "#         print(self.time_slides)\n",
    "        self.time_slides = sorted(self.time_slides,key=getfirst)\n",
    "        \n",
    "        #concat time period\n",
    "        self.time_slides = [Interval(item[0],item[1],lower_closed=True, upper_closed=True) for item in self.time_slides]\n",
    "#         print(len(self.time_slides))\n",
    "        while True:\n",
    "            big_flag = 0\n",
    "            for i in range(len(self.time_slides)):\n",
    "                \n",
    "                flag = 0\n",
    "                for j in range(len(self.time_slides)):\n",
    "#                     print((i,j))\n",
    "                    if i ==len(self.time_slides)-1 and j == len(self.time_slides)-1:\n",
    "                        big_flag =1\n",
    "                    if i==j:\n",
    "                        continue      \n",
    "                    if self.time_slides[i].overlaps(self.time_slides[j]):\n",
    "                        interval_merge = self.time_slides[i].join(self.time_slides[j])\n",
    "                        self.time_slides[i] = interval_merge\n",
    "                        del self.time_slides[j]\n",
    "\n",
    "                        flag = 1\n",
    "                        break                \n",
    "                if flag == 1:\n",
    "                    break\n",
    "            if big_flag==1:\n",
    "                break\n",
    "        self.time_slides = [(item.lower_bound,item.upper_bound) for item in self.time_slides]\n",
    "#         print(self.time_slides)\n",
    "        \n",
    "                    \n",
    "                    \n",
    "                \n",
    "\n",
    "            \n",
    "        \n",
    "      \n",
    "    #新归纳session的url汇聚并排序计算\n",
    "    def urls_cal(self):\n",
    "        for item in self.sessions:\n",
    "            urls = item['urls']\n",
    "            for url in urls:\n",
    "                if url not in list(self.urls.keys()):\n",
    "                    self.urls[url] = 1\n",
    "                else:\n",
    "                    self.urls[url] += 1\n",
    "        #sorted\n",
    "        self.url_number = sorted(self.urls.items(),key=lambda item:item[1],reverse=True)\n",
    "    \n",
    "    #计算url重要系数\n",
    "    def urls_weight(self):\n",
    "        total = np.array([item[1] for item in self.url_number]).sum()*1.0\n",
    "        for item in self.url_number:\n",
    "            self.url_weight[item[0]] = item[1]*1.0/total\n",
    "                \n",
    "#         self.url_weight = [(item[0],item[1]*1.0/total) for item in self.url_number]\n",
    "        self.url_unique = [item[0] for item in self.url_number]\n",
    "        \n",
    "    #IP归纳\n",
    "    def ips_cal(self):\n",
    "        for item in self.sessions:\n",
    "            if item['IP'] not in self.IPs:\n",
    "                self.IPs.append(item['IP'])\n",
    "    \n",
    "    #cert归纳\n",
    "    def cert_cal(self):\n",
    "        for item in self.sessions:\n",
    "            if item['cert'] not in self.certs:\n",
    "                self.certs.append(item['cert'])\n",
    "        self.certs = [item for item in self.certs if item is not 0]\n",
    "    \n",
    "    #抛弃低频urls\n",
    "    def url_clean(self):\n",
    "        if len(self.url_number) <=5:\n",
    "            return\n",
    "        self.url_number = self.url_number[:5]\n",
    "        self.url_unique = [item[0] for item in self.url_number]\n",
    "        self.url_weight = {}\n",
    "        self.urls_weight()\n",
    "        \n",
    "    #ip_weigth\n",
    "    def ip_weight_cal(self):\n",
    "        self.ip_weight = {}\n",
    "        for session in self.sessions:\n",
    "            if session['IP'] not in list(self.ip_weight.keys()):\n",
    "                self.ip_weight[session['IP']] = 1.0/(len(self.sessions)*1.0)\n",
    "            else:\n",
    "                self.ip_weight[session['IP']] += 1.0/(len(self.sessions)*1.0)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    #cert_weigth\n",
    "    def cert_weight_cal(self):\n",
    "        self.cert_weight = {}\n",
    "        count = 0\n",
    "        for session in self.sessions:\n",
    "            if session['cert'] != 0 and session['cert'] not in list(self.cert_weight.keys()):\n",
    "                self.cert_weight[session['cert']] = 1\n",
    "                count +=1\n",
    "            elif session['cert'] != 0 and session['cert'] in list(self.cert_weight.keys()):\n",
    "                self.cert_weight[session['cert']] += 1\n",
    "                count +=1\n",
    "        if count != 0:\n",
    "            for key in list(self.cert_weight.keys()):\n",
    "                self.cert_weight[key] = self.cert_weight[key]/(count*1.0)\n",
    "    \n",
    "    \n",
    "    #更新cert_clt\n",
    "    def update(self):\n",
    "        self.urls_cal()\n",
    "        self.urls_weight()\n",
    "        self.ips_cal()\n",
    "        self.cert_cal()\n",
    "        self.seq_cal()\n",
    "        self.image_cal()\n",
    "        self.label_cal()\n",
    "        self.tag_cal()\n",
    "        self.static_cal()\n",
    "        self.url_clean()\n",
    "        self.all_url_cal()\n",
    "        self.fn_cal()\n",
    "        \n",
    "        self.cert_weight_cal()\n",
    "        self.ip_weight_cal()\n",
    "        \n",
    "#         self.time_cal()\n",
    "                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_matrix_gen(clusters):\n",
    "    def cert_sim(clt1,clt2):\n",
    "        if len(list(clt1.cert_weight.keys())) == 0 or len(list(clt2.cert_weight.keys())) == 0:\n",
    "            return 0\n",
    "        cross_keys = list(set(list(clt1.cert_weight.keys()))&set(list(clt2.cert_weight.keys())))\n",
    "        score = 0.0\n",
    "        for key in cross_keys:\n",
    "            score += clt1.cert_weight[key]*clt2.cert_weight[key]\n",
    "        return score\n",
    "    def ip_sim(clt1,clt2):\n",
    "        if len(list(clt1.ip_weight.keys())) == 0 or len(list(clt2.ip_weight.keys())) == 0:\n",
    "            return 0\n",
    "        cross_keys = list(set(list(clt1.ip_weight.keys()))&set(list(clt2.ip_weight.keys())))\n",
    "        score = 0.0\n",
    "        for key in cross_keys:\n",
    "            score += clt1.ip_weight[key]*clt2.ip_weight[key]\n",
    "        return score\n",
    "    \n",
    "    def url_sim(clt1,clt2):\n",
    "        if len(clt1.url_unique) == 0 or len(clt2.url_unique) == 0:\n",
    "            return 0\n",
    "        \n",
    "        overlaps = list(set(clt1.url_unique)&set(clt2.url_unique))\n",
    "        if len(overlaps) is 0:\n",
    "            return 0\n",
    "        res =  np.array([clt1.url_weight[key]*clt2.url_weight[key] for key in overlaps]).sum() / len(overlaps)\n",
    "#         print(res)\n",
    "        if res <0.3:\n",
    "            res = 0\n",
    "        return res\n",
    "#     def cert_sim(clt1,clt2):\n",
    "#         if len(clt1.certs) == 0 or len(clt2.certs) == 0:\n",
    "#             return 0\n",
    "#         return len(list(set(clt1.certs)&set(clt2.certs)))\n",
    "    \n",
    "#     def url_sim(clt1,clt2):\n",
    "#         if len(clt1.url_unique) == 0 or len(clt2.url_unique) == 0:\n",
    "#             return 0\n",
    "        \n",
    "#         overlaps = list(set(clt1.url_unique)&set(clt2.url_unique))\n",
    "#         if len(overlaps) is 0:\n",
    "#             return 0\n",
    "#         res =  np.array([clt1.url_weight[key]*clt2.url_weight[key] for key in overlaps]).sum() / len(overlaps)\n",
    "# #         print(res)\n",
    "#         if res <0.3:\n",
    "#             res = 0\n",
    "#         return res\n",
    "    \n",
    "    def time_sim(clt1,clt2):\n",
    "        clt1_time = [Interval(item[0],item[1],lower_closed=True, upper_closed=True) for item in clt1.time_slides]\n",
    "        clt2_time = [Interval(item[0],item[1],lower_closed=True, upper_closed=True) for item in clt2.time_slides]\n",
    "        count = 0\n",
    "        for clt1_t in clt1_time:\n",
    "            for clt2_t in clt2_time:\n",
    "                if clt1_t.overlaps(clt2_t):\n",
    "                    count+=1\n",
    "        if count <=7:\n",
    "            count = 0\n",
    "        return count\n",
    "    \n",
    "    mat = np.zeros((len(clusters),len(clusters)))\n",
    "    for i in range(len(clusters)):\n",
    "        for j in range(i,len(clusters)):\n",
    "            if i==j:\n",
    "                continue\n",
    "            mat[i][j] = url_sim(clusters[i],clusters[j]) + cert_sim(clusters[i],clusters[j]) + ip_sim(clusters[i],clusters[j]) #+ time_sim(clusters[i],clusters[j])\n",
    "            if i!=j:\n",
    "                mat[j][i] = mat[i][j]\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clt_analysis(clts, adj_matrix):\n",
    "    inter_per_list = []\n",
    "    max_indx_list = []\n",
    "    outer_score_list_all = []\n",
    "    for i in range(adj_matrix.shape[0]):\n",
    "        if clts[i].tag == 0:\n",
    "            continue\n",
    "        inter_score = 0.0\n",
    "        outer_score = 0.0\n",
    "#         inter_score_list = [0.0 for i in range(10)]\n",
    "        outer_score_list = [0.0 for i in range(class_num)]\n",
    "        \n",
    "        for j in range(adj_matrix.shape[0]):\n",
    "            if i==j or clts[j].tag == 0:\n",
    "                continue\n",
    "            if clts[i].label == clts[j].label:\n",
    "                inter_score += adj_matrix[i][j]\n",
    "                outer_score_list[int(clts[i].label)] += adj_matrix[i][j]\n",
    "            else:\n",
    "                outer_score_list[int(clts[j].label)] += adj_matrix[i][j]\n",
    "                outer_score += adj_matrix[i][j]\n",
    "        if (inter_score+outer_score) != 0:\n",
    "            inter_per_list.append(inter_score/(inter_score+outer_score))\n",
    "        else:\n",
    "            inter_per_list.append(-1)\n",
    "            \n",
    "#         outer_score_list[clts[i].label] = adj_matrix[i][i]\n",
    "        max_indx_list.append(np.argmax(np.array(outer_score_list)))\n",
    "        outer_score_list_all.append(outer_score_list)\n",
    "    outer_clt = []\n",
    "    for i in range(len(inter_per_list)):\n",
    "        if inter_per_list[i] <=0.5 and inter_per_list[i] >= 0:\n",
    "            outer_clt.append((i,clts[i]))\n",
    "    max_indx_list = [(outer_score_list_all[i][clts[i].label],outer_score_list_all[i], len(clts[i].sessions)) for i in range(len(max_indx_list)) if max_indx_list[i] != int(clts[i].label) and np.array(max_indx_list[i]).sum()>0]\n",
    "    return outer_clt,max_indx_list\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhc_iso(clts,labels):\n",
    "    def time_sim(clt1,clt2):\n",
    "        clt1_time = [Interval(item[0],item[1],lower_closed=True, upper_closed=True) for item in clt1.time_slides]\n",
    "        clt2_time = [Interval(item[0],item[1],lower_closed=True, upper_closed=True) for item in clt2.time_slides]\n",
    "        count = 0\n",
    "        for clt1_t in clt1_time:\n",
    "            for clt2_t in clt2_time:\n",
    "                if clt1_t.overlaps(clt2_t):\n",
    "                    count+=1\n",
    "        return count\n",
    "    \n",
    "    #deal with the isolated clts only\n",
    "    zero_idx = [i for i in range(len(labels)) if labels[i] == -1]\n",
    "    \n",
    "#     sum_v = np.sum(adj_matrix,axis=0).reshape((adj_matrix.shape[0],1)) - np.array([adj_matrix[i][i] for i in range(adj_matrix.shape[0])]).reshape((adj_matrix.shape[0],1))\n",
    "#     zero_idx = [i for i in range(sum_v.shape[0]) if int(sum_v[i]) == 0]\n",
    "#     adj_matrix_new = adj_matrix.copy()\n",
    "    for idx in zero_idx:\n",
    "        #for each clt, we calculate the time sim with other clts\n",
    "        score_list = [0.0 for x in range(class_num)]\n",
    "        for j in range(len(clts)):\n",
    "            if j == idx:\n",
    "                continue\n",
    "            score = time_sim(clts[idx],clts[j])\n",
    "            score_list[labels[j]] += score\n",
    "        pred_label = np.argmax(np.array(score_list))\n",
    "        labels[idx] = pred_label\n",
    "        \n",
    "    return labels\n",
    "        \n",
    "def check_zero_label(adj_matrix,labels):\n",
    "    idxs = []\n",
    "    for i in range(adj_matrix.shape[0]):\n",
    "        if labels[i] == -1:\n",
    "            continue\n",
    "        flag = 1\n",
    "        count = 0\n",
    "        for j in range(adj_matrix.shape[0]):\n",
    "            if i==j:\n",
    "                continue\n",
    "            if labels[j] == -1:\n",
    "                continue\n",
    "            if labels[i] == labels[j] and adj_matrix[i][j] != 0:\n",
    "                flag = 0\n",
    "            elif labels[i] != labels[j] and adj_matrix[i][j] != 0:\n",
    "                count += adj_matrix[i][j]\n",
    "        if flag and count != 0:\n",
    "            idxs.append(i)\n",
    "    return idxs\n",
    "            \n",
    "def zero_adj(zero_idxs,clts, adj_matrix):\n",
    "    for idx in zero_idxs:\n",
    "        if clts[idx].tag == 0:\n",
    "            continue\n",
    "        cand = clts[idx]\n",
    "        score_list = [0.0 for i in range(class_num)]\n",
    "        for i in range(len(clts)):\n",
    "            if i==idx:\n",
    "                continue\n",
    "            score_list[clts[i].label] += adj_matrix[idx][i]\n",
    "        prob = np.array(softmax(score_list))\n",
    "        if np.argmax(prob) != clts[idx].label and prob[np.argmax(prob)] >= 0.9:\n",
    "            clts[idx].label = np.argmax(prob)\n",
    "    return clts\n",
    "        \n",
    "\n",
    "def check_zero(adj_matrix):\n",
    "    sum_v = np.sum(adj_matrix,axis=0).reshape((adj_matrix.shape[0],1)) - np.array([adj_matrix[i][i] for i in range(adj_matrix.shape[0])]).reshape((adj_matrix.shape[0],1))\n",
    "    zero_idx = [i for i in range(sum_v.shape[0]) if int(sum_v[i]) == 0]\n",
    "    print(len(zero_idx))\n",
    "    return zero_idx\n",
    "\n",
    "#adjust the label of outer clts\n",
    "def clt_adj(clts,o_clts):\n",
    "    def cert_sim(clt1,clt2):\n",
    "        if len(clt1.certs) == 0 or len(clt2.certs) == 0:\n",
    "            return 0\n",
    "        return len(list(set(clt1.certs)&set(clt2.certs)))\n",
    "    def ip_sim(clt1,clt2):\n",
    "        return len(list(set(clt1.IPs)&set(clt2.IPs)))\n",
    "    for o_idx,o_clt in o_clts:\n",
    "        clss = [0.0 for i in range(class_num)]\n",
    "        for i in range(len(clts)):\n",
    "            if i==o_idx:\n",
    "                continue\n",
    "            sim = cert_sim(o_clt,clts[i]) + ip_sim(o_clt,clts[i])\n",
    "            clss[clts[i].label] += sim\n",
    "        if np.array(clss).max() < 2:\n",
    "            print(\"{} no need for adjustment\".format(o_idx))\n",
    "            continue\n",
    "        if np.argmax(np.array(clss)) != o_clt.label:\n",
    "            print(\"{}: {} -> {}\".format(o_idx, o_clt.label,np.argmax(np.array(clss))))\n",
    "            clts[o_idx].label = np.argmax(np.array(clss))\n",
    "            \n",
    "                           \n",
    "    return clts\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x - np.max(x))/(np.sum(np.exp(x - np.max(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "\n",
    "\n",
    "def spread(clts,adj_matrix,labels):\n",
    "    #predict the unlabel clts\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] != -1:\n",
    "            continue\n",
    "        score_list = [0.0 for x in range(class_num)]\n",
    "        for j in range(len(labels)):\n",
    "            if i==j: #ignore the self-node\n",
    "                continue\n",
    "            if labels[j] == -1: #ignore the unlabel testing nodes\n",
    "                continue\n",
    "            score_list[labels[j]] += adj_matrix[i][j]\n",
    "        if np.array(score_list).sum() == 0:\n",
    "            continue\n",
    "        score_list = np.array(softmax(score_list))\n",
    "        pred_label = np.argmax(score_list)\n",
    "        #tag the predicted label\n",
    "        labels[i] = pred_label\n",
    "#         print(pred_label)\n",
    "    return labels\n",
    "\n",
    "# def acc_per_round(clts,labels):\n",
    "# #     gt_labels = [[clt.label]*len(clt.sessions) for clt in clts if clt.tag==0 and clt.label!=-1]\n",
    "#     gt_labels = [[clts[i].label]*len(clts[i].sessions) for i in range(len(clts)) if clts[i].tag==0 and labels[i]!=-1]\n",
    "#     labels_pred = [[labels[i]]*len(clts[i].sessions) for i in range(len(clts)) if clts[i].tag==0 and labels[i]!=-1]\n",
    "# #     gt_labels = [item0 for item in gt_labels for item0 in item]\n",
    "# #     labels_pred = [item0 for item in labels_pred for item0 in item]\n",
    "#     acc = accuracy_score(gt_labels,labels_pred)\n",
    "#     pre = recall_score(gt_labels,labels_pred,average=\"weighted\")\n",
    "#     rec = precision_score(gt_labels,labels_pred, average=\"weighted\")\n",
    "#     f1 = f1_score(gt_labels,labels_pred,average=\"weighted\")\n",
    "#     return acc,pre,rec,f1\n",
    "\n",
    "def acc_per_round(clts,labels):\n",
    "#     gt_labels = [[clt.label]*len(clt.sessions) for clt in clts if clt.tag==0 and clt.label!=-1]\n",
    "    gt_labels = [clts[i].label for i in range(len(clts)) if clts[i].tag==0 and labels[i]!=-1]\n",
    "    labels_pred = [labels[i] for i in range(len(clts)) if clts[i].tag==0 and labels[i]!=-1]\n",
    "#     gt_labels = [item0 for item in gt_labels for item0 in item]\n",
    "#     labels_pred = [item0 for item in labels_pred for item0 in item]\n",
    "    acc = accuracy_score(gt_labels,labels_pred)\n",
    "    pre = recall_score(gt_labels,labels_pred,average=\"weighted\")\n",
    "    rec = precision_score(gt_labels,labels_pred, average=\"weighted\")\n",
    "    f1 = f1_score(gt_labels,labels_pred,average=\"weighted\")\n",
    "    return acc,pre,rec,f1\n",
    "\n",
    "# def acc_per_round(clts,labels):\n",
    "#     gt_labels = [clt.label for clt in clts]\n",
    "#     acc = accuracy_score(gt_labels,labels)\n",
    "#     pre = recall_score(gt_labels,labels,average=\"weighted\")\n",
    "#     rec = precision_score(gt_labels,labels, average=\"weighted\")\n",
    "#     f1 = f1_score(gt_labels,labels,average=\"weighted\")\n",
    "#     return acc,pre,rec,f1\n",
    "    \n",
    "#     acc = 0\n",
    "#     al = 0\n",
    "#     for i in range(len(labels)):\n",
    "#         if labels[i]==-1 or clts[i].tag == 1:\n",
    "#             continue\n",
    "#         al += 1\n",
    "#         if labels[i] == clts[i].label:\n",
    "#             acc += 1\n",
    "#     return acc*1.0/(al*1.0)\n",
    "\n",
    "def aggregation(clts,mat,labels):\n",
    "    #ip,cert,url weight aggragation\n",
    "    def agg_weight(clt_c,clt_n,weight):\n",
    "        #ip agg\n",
    "        for ip in list(clt_n.ip_weight.keys()):\n",
    "            if ip not in list(clt_c.ip_weight.keys()):\n",
    "                clt_c.ip_weight[ip] = clt_n.ip_weight[ip] * weight\n",
    "            else:\n",
    "                clt_c.ip_weight[ip] += clt_c.ip_weight[ip] * clt_n.ip_weight[ip] * weight\n",
    "        #cert agg\n",
    "        for cert in list(clt_n.cert_weight.keys()):\n",
    "            if cert not in list(clt_c.cert_weight.keys()):\n",
    "                clt_c.cert_weight[cert] = clt_n.cert_weight[cert] * weight\n",
    "            else:\n",
    "                clt_c.cert_weight[cert] += clt_c.cert_weight[cert] * clt_n.cert_weight[cert] * weight\n",
    "                \n",
    "        #url agg\n",
    "        for url in list(clt_n.url_weight.keys()):\n",
    "            if url not in list(clt_c.url_weight.keys()):\n",
    "                clt_c.url_weight[url] = clt_n.url_weight[url] * weight\n",
    "            else:\n",
    "                clt_c.url_weight[url] += clt_c.url_weight[url] * clt_n.url_weight[url] * weight\n",
    "                \n",
    "        \n",
    "    \n",
    "    #aggregate the labeled nodes\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == -1:\n",
    "            continue\n",
    "        #aggragate the labeled nodes from same class\n",
    "        softmax_mat = []\n",
    "        for k in range(len(labels)):\n",
    "            if labels[k]!=-1 and labels[k]==labels[i]:\n",
    "                softmax_mat.append(mat[i][k])\n",
    "            else:\n",
    "                softmax_mat.append(0) \n",
    "        for j in range((len(labels))):\n",
    "            if i==j or labels[i] != labels[j]:\n",
    "                continue\n",
    "            #we aggregate the labled nodes from same class(so as to calculate the softmax)\n",
    "            agg_weight(clts[i],clts[j],softmax(softmax_mat)[j]) \n",
    "    #update the edge with the unlabeled nodes\n",
    "    adj_matrix = adj_matrix_gen(clts)\n",
    "    return adj_matrix\n",
    "            \n",
    "def label_update(clts,labels):\n",
    "    labels = []\n",
    "    for clt in clts:\n",
    "        if clt.tag == 0:\n",
    "            labels.append(-1)\n",
    "        else:\n",
    "            labels.append(clt.label)\n",
    "    labels = np.array(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_init(mode,reverse = False, test_rate = 0):\n",
    "    #load traning data from net_A\n",
    "    if reverse:\n",
    "        with open(\"data/ScenarioB.pkl\",'rb') as file:\n",
    "            dataT  = pickle.loads(file.read())\n",
    "    else:\n",
    "        with open(\"data/ScenarioA.pkl\",'rb') as file:\n",
    "            dataT  = pickle.loads(file.read())\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for i in range(len(dataT.label)):\n",
    "        tmp = {}\n",
    "        tmp['IP'] = dataT.static_feature[i][0]\n",
    "        if len(dataT.cert_number[i]) is 0:\n",
    "            tmp['cert'] = 0\n",
    "        else:\n",
    "            tmp['cert'] = dataT.cert_number[i][0]\n",
    "        tmp['urls'] = [item for item in dataT.urls[i] if item != '0']\n",
    "        tmp['static_features'] = np.array([float(item) for item in dataT.static_feature[i][1:-2]])\n",
    "        \n",
    "        tmp['seq'] = dataT.seq_matirx[i]\n",
    "        tmp['label'] = dataT.label[i]\n",
    "        tmp['time'] = (float(dataT.static_feature[i][-2]),float(dataT.static_feature[i][-1]))\n",
    "        tmp['image'] = dataT.image[i]\n",
    "        tmp['fn'] = dataT.fn[i]\n",
    "        tmp['tag'] = 1\n",
    "\n",
    "        data_list.append(tmp)\n",
    "    print(\"{} sessions loaded from training set.\".format(len(data_list)))\n",
    "\n",
    "    IP_clusters = []\n",
    "    used_sessions = []\n",
    "\n",
    "    #inital IP clusters\n",
    "    for i in range(len(data_list)):\n",
    "        session = data_list[i]\n",
    "        IP = session['IP']\n",
    "        if len(IP_clusters) == 0:\n",
    "            used_sessions.append(i)\n",
    "            clt = cert_cluster()\n",
    "            clt.sessions.append(session)\n",
    "            clt.update()\n",
    "            IP_clusters.append(clt)\n",
    "        else:\n",
    "            d = 0\n",
    "            for j in range(len(IP_clusters)):\n",
    "                if IP in IP_clusters[j].IPs:\n",
    "                    IP_clusters[j].sessions.append(session)\n",
    "                    IP_clusters[j].update()\n",
    "                    used_sessions.append(i)\n",
    "                    d = 1\n",
    "                    break\n",
    "\n",
    "            if d == 0:\n",
    "                used_sessions.append(i)\n",
    "                clt = cert_cluster()\n",
    "                clt.sessions.append(session)\n",
    "                clt.update()\n",
    "                IP_clusters.append(clt)\n",
    "\n",
    "    data_list = [data_list[i] for i in range(len(data_list)) if i not in used_sessions]\n",
    "    for item in IP_clusters:\n",
    "        item.time_cal()\n",
    "    if mode == 'cross':\n",
    "        print(\"{} clusters are initialized for training set.\".format(len(IP_clusters)))\n",
    "        \n",
    "    if mode == 'cross':\n",
    "        if reverse:\n",
    "            with open(\"data/ScenarioA.pkl\",'rb') as file:\n",
    "                dataT  = pickle.loads(file.read())\n",
    "        else:\n",
    "            with open(\"data/ScenarioB.pkl\",'rb') as file:\n",
    "                dataT  = pickle.loads(file.read())\n",
    "\n",
    "        for i in range(len(dataT.label)):\n",
    "            tmp = {}\n",
    "            tmp['IP'] = dataT.static_feature[i][0]\n",
    "            if len(dataT.cert_number[i]) is 0:\n",
    "                tmp['cert'] = 0\n",
    "            else:\n",
    "                tmp['cert'] = dataT.cert_number[i][0]\n",
    "            tmp['urls'] = [item for item in dataT.urls[i] if item != '0']\n",
    "            tmp['static_features'] = np.array([float(item) for item in dataT.static_feature[i][1:-2]])\n",
    "            tmp['seq'] = dataT.seq_matirx[i]\n",
    "            tmp['label'] = dataT.label[i]\n",
    "            tmp['time'] = (float(dataT.static_feature[i][-2]),float(dataT.static_feature[i][-1]))\n",
    "            tmp['image'] = dataT.image[i]\n",
    "            tmp['fn'] = dataT.fn[i]\n",
    "            tmp['tag'] = 0\n",
    "\n",
    "            data_list.append(tmp)\n",
    "        print(\"{} sessions loaded from testing set.\".format(len(data_list)))\n",
    "\n",
    "        IP_clusters_poor = []\n",
    "        used_sessions = []\n",
    "\n",
    "        #inital IP clusters\n",
    "        for i in range(len(data_list)):\n",
    "            session = data_list[i]\n",
    "            IP = session['IP']\n",
    "            if len(IP_clusters_poor) == 0:\n",
    "                used_sessions.append(i)\n",
    "                clt = cert_cluster()\n",
    "                clt.sessions.append(session)\n",
    "                clt.update()\n",
    "                IP_clusters_poor.append(clt)\n",
    "            else:\n",
    "                d = 0\n",
    "                for j in range(len(IP_clusters_poor)):\n",
    "                    if IP in IP_clusters_poor[j].IPs:\n",
    "                        IP_clusters_poor[j].sessions.append(session)\n",
    "                        IP_clusters_poor[j].update()\n",
    "                        used_sessions.append(i)\n",
    "                        d = 1\n",
    "                        break\n",
    "\n",
    "                if d == 0:\n",
    "                    used_sessions.append(i)\n",
    "                    clt = cert_cluster()\n",
    "                    clt.sessions.append(session)\n",
    "                    clt.update()\n",
    "                    IP_clusters_poor.append(clt)\n",
    "\n",
    "        data_list = [data_list[i] for i in range(len(data_list)) if i not in used_sessions]\n",
    "        for item in IP_clusters_poor:\n",
    "            item.time_cal()\n",
    "        IP_clusters.extend(IP_clusters_poor)\n",
    "        \n",
    "        print(\"{} clusters are initialized for testing set.\".format(len(IP_clusters_poor)))\n",
    "        print(\"{} nodes are included in the initialized graph.\".format(len(IP_clusters)))\n",
    "        return IP_clusters\n",
    "    else:\n",
    "        #construct testing set\n",
    "        label_idx_dict = {}\n",
    "        for i in range(len(IP_clusters)):\n",
    "            clt = IP_clusters[i]\n",
    "            if clt.label not in list(label_idx_dict.keys()):\n",
    "                label_idx_dict[clt.label] = [i]\n",
    "            else:\n",
    "                label_idx_dict[clt.label].append(i)\n",
    "        test_idx = []\n",
    "        for key in list(label_idx_dict.keys()):\n",
    "            idxs = np.arange(len(label_idx_dict[key]))\n",
    "#             print(idxs)\n",
    "            np.random.shuffle(idxs)\n",
    "            idxs = idxs[:int(idxs.shape[0]*test_rate)]\n",
    "            test_idx.extend(np.array(label_idx_dict[key])[list(idxs)])\n",
    "        for idx in test_idx:\n",
    "            IP_clusters[idx].tag = 0\n",
    "        print(\"{} clusters are initialized for training set.\".format(len(IP_clusters) - len(test_idx)))\n",
    "        print(\"{} clusters are initialized for testing set.\".format(len(test_idx)))\n",
    "        print(\"{} nodes are included in the initialized graph.\".format(len(IP_clusters)))\n",
    "        return IP_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_mat_init(IP_clusters):\n",
    "    adj_matrix = adj_matrix_gen(IP_clusters)\n",
    "    labels = []\n",
    "    for clt in IP_clusters:\n",
    "        if clt.tag == 0:\n",
    "            labels.append(-1)\n",
    "        else:\n",
    "            labels.append(clt.label)\n",
    "    labels = np.array(labels)\n",
    "    return adj_matrix,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spread(clts,adj_matrix,labels,th=None,mode = 'Normal'):\n",
    "    #predict the unlabel clts\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] != -1:\n",
    "            continue\n",
    "        score_list = [0.0 for x in range(class_num)]\n",
    "            \n",
    "        for j in range(len(labels)):\n",
    "            if i==j: #ignore the self-node\n",
    "                continue\n",
    "            if labels[j] == -1: #ignore the unlabel testing nodes\n",
    "                continue\n",
    "            score_list[labels[j]] += adj_matrix[i][j]\n",
    "        if np.array(score_list).sum() == 0:\n",
    "            continue\n",
    "#         print(score_list)\n",
    "        score_list = np.array(softmax(score_list))\n",
    "        \n",
    "        \n",
    "#         print(score_list)\n",
    "        # if score_list.max() < th:\n",
    "        #     continue\n",
    "#         print(\"maxvalue: {} maxlabel: {} gt: {}\".format(max(score_list),np.argmax(score_list),clts[i].label))\n",
    "        \n",
    "        \n",
    "        pred_label = np.argmax(score_list)\n",
    "        #tag the predicted label\n",
    "        labels[i] = pred_label\n",
    "#         print(pred_label)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4429 sessions loaded from training set.\n",
      "918 clusters are initialized for training set.\n",
      "2751 sessions loaded from testing set.\n",
      "577 clusters are initialized for testing set.\n",
      "1495 nodes are included in the initialized graph.\n",
      "*** Confused nodes should be adjusted to the ground true labels.***\n",
      "81 no need for adjustment\n",
      "107: 1 -> 11\n",
      "109 no need for adjustment\n",
      "118 no need for adjustment\n",
      "127 no need for adjustment\n",
      "146 no need for adjustment\n",
      "151 no need for adjustment\n",
      "152 no need for adjustment\n",
      "154: 4 -> 18\n",
      "155: 4 -> 18\n",
      "160: 4 -> 14\n",
      "161 no need for adjustment\n",
      "163 no need for adjustment\n",
      "164 no need for adjustment\n",
      "167 no need for adjustment\n",
      "172 no need for adjustment\n",
      "173 no need for adjustment\n",
      "189 no need for adjustment\n",
      "192 no need for adjustment\n",
      "193: 4 -> 19\n",
      "194 no need for adjustment\n",
      "195: 16 -> 19\n",
      "206 no need for adjustment\n",
      "208 no need for adjustment\n",
      "209 no need for adjustment\n",
      "210: 4 -> 19\n",
      "211: 18 -> 4\n",
      "213: 4 -> 18\n",
      "214: 4 -> 18\n",
      "219: 12 -> 5\n",
      "247: 12 -> 5\n",
      "248: 12 -> 5\n",
      "252: 5 -> 18\n",
      "270: 6 -> 7\n",
      "288: 11 -> 7\n",
      "349 no need for adjustment\n",
      "362 no need for adjustment\n",
      "375 no need for adjustment\n",
      "387 no need for adjustment\n",
      "422 no need for adjustment\n",
      "427 no need for adjustment\n",
      "434 no need for adjustment\n",
      "495: 9 -> 19\n",
      "512 no need for adjustment\n",
      "532: 10 -> 1\n",
      "536 no need for adjustment\n",
      "538: 10 -> 19\n",
      "539 no need for adjustment\n",
      "543: 11 -> 7\n",
      "558: 11 -> 10\n",
      "570 no need for adjustment\n",
      "577 no need for adjustment\n",
      "702: 17 -> 14\n",
      "727: 15 -> 11\n",
      "729 no need for adjustment\n",
      "734: 15 -> 19\n",
      "736 no need for adjustment\n",
      "738 no need for adjustment\n",
      "741 no need for adjustment\n",
      "743 no need for adjustment\n",
      "747: 15 -> 11\n",
      "754 no need for adjustment\n",
      "771 no need for adjustment\n",
      "772 no need for adjustment\n",
      "774 no need for adjustment\n",
      "775 no need for adjustment\n",
      "776 no need for adjustment\n",
      "778 no need for adjustment\n",
      "780 no need for adjustment\n",
      "781 no need for adjustment\n",
      "783 no need for adjustment\n",
      "784 no need for adjustment\n",
      "786 no need for adjustment\n",
      "793 no need for adjustment\n",
      "794: 17 -> 14\n",
      "795: 17 -> 14\n",
      "805 no need for adjustment\n",
      "810 no need for adjustment\n",
      "839 no need for adjustment\n",
      "856: 18 -> 4\n",
      "868: 19 -> 14\n",
      "883 no need for adjustment\n",
      "887 no need for adjustment\n",
      "888 no need for adjustment\n",
      "161 no need for adjustment\n",
      "163 no need for adjustment\n",
      "164 no need for adjustment\n",
      "167 no need for adjustment\n",
      "172 no need for adjustment\n",
      "173 no need for adjustment\n",
      "194 no need for adjustment\n",
      "209 no need for adjustment\n",
      "422 no need for adjustment\n",
      "536 no need for adjustment\n",
      "729 no need for adjustment\n",
      "738 no need for adjustment\n",
      "741 no need for adjustment\n",
      "772 no need for adjustment\n",
      "774 no need for adjustment\n",
      "780 no need for adjustment\n",
      "783 no need for adjustment\n",
      "805 no need for adjustment\n",
      "888 no need for adjustment\n",
      "\n",
      "Starting to propagate.\n",
      "Epoch : 0  Acc: 0.912109375 pre: 0.912109375 rec: 0.9182280593619162 f1: 0.9151584900234903.\n",
      "Epoch : 1  Acc: 0.912109375 pre: 0.912109375 rec: 0.9182280593619162 f1: 0.9151584900234903.\n",
      "Epoch : 2  Acc: 0.9122807017543859 pre: 0.9122807017543859 rec: 0.9183874588563374 f1: 0.9153238948214207.\n",
      "Testing Acc: 0.9081455805892548 pre: 0.9081455805892548 rec: 0.9123107608115386 f1: 0.907602401769798 after 3 epochs.\n"
     ]
    }
   ],
   "source": [
    "#initialize training set and testing set ->IP_clusters\n",
    "IP_clusters = data_init(mode='cross',reverse=False,test_rate= 0.3)\n",
    "#adj_matrix and labels initialization\n",
    "adj_matrix,labels = label_mat_init(IP_clusters)\n",
    "#check out outer clusters\n",
    "outer_clt,max_indx_list = clt_analysis(IP_clusters,adj_matrix)\n",
    "\n",
    "#update labels\n",
    "labels = label_update(IP_clusters,labels)\n",
    "#check out isolate clusters\n",
    "zero_idx = check_zero_label(adj_matrix,labels)\n",
    "zero_clts = [(i, IP_clusters[i]) for i in range(len(IP_clusters)) if i in zero_idx]\n",
    "adj_cand_clts = outer_clt + zero_clts\n",
    "#adjust iso clts to the gt classes\n",
    "print(\"*** Confused nodes should be adjusted to the ground true labels.***\")\n",
    "clts_adj = clt_adj(IP_clusters,adj_cand_clts)\n",
    "\n",
    "#start to propagate\n",
    "print()\n",
    "print(\"Starting to propagate.\")\n",
    "epoch = 3 # A small epoch can achieve satisfactory performance and prevent from overfitting\n",
    "for e in range(epoch):\n",
    "    # nodes aggregate info from neighbours and update the adjoin edges\n",
    "    adj_matrix = aggregation(clts_adj,adj_matrix,labels)\n",
    "    # update the labels\n",
    "#     labels = label_update(clts_adj,labels)\n",
    "    # spread label info to the unlabeled neighbours\n",
    "    labels = spread(clts_adj,adj_matrix,labels)\n",
    "    # performing measurement in testing set\n",
    "    acc,pre,rec,f1 = acc_per_round(clts_adj,labels)\n",
    "    f1 = 2*pre*rec/(pre+rec)\n",
    "    print(\"Epoch : {}  Acc: {} pre: {} rec: {} f1: {}.\".format(e,acc,pre,rec,f1))\n",
    "# deal with the isolated nodes, aggregate by time info\n",
    "labels = enhc_iso(clts_adj,labels)\n",
    "f1 = 2*pre*rec/(pre+rec)\n",
    "acc,pre,rec,f1 = acc_per_round(clts_adj,labels)\n",
    "# acc after propagation\n",
    "print(\"Testing Acc: {} pre: {} rec: {} f1: {} after {} epochs.\".format(acc,pre,rec,f1,epoch))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.9081455805892548 pre: 0.9081455805892548 rec: 0.9123107608115386 f1: 0.9102234057615487 after 3 epochs.\n"
     ]
    }
   ],
   "source": [
    "acc,pre,rec,f1 = acc_per_round(clts_adj,labels)\n",
    "f1 = 2*pre*rec/(pre+rec)\n",
    "# acc after propagation\n",
    "print(\"Testing Acc: {} pre: {} rec: {} f1: {} after {} epochs.\".format(acc,pre,rec,f1,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
